{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORkYyaTvpR+DwaQ74QLHtd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lXVO8MRZMxoQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#unziping train dataest\n","!mkdir data\n","!mkdir data/output\n","!mkdir data/model\n","!mkdir /content/data/data\n","!unzip /content/drive/MyDrive/DIV2K_train_HR.zip -d /content/data/data"],"metadata":{"id":"o-TkKf-R34ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#unzipping test dataset\n","!unzip /content/drive/MyDrive/set5.zip -d /content/data/"],"metadata":{"id":"qFUHxkM6-fOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/set14.zip -d /content/data/"],"metadata":{"id":"Sa6HKuTgJ9lD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","\n","#set5 testset preparation\n","\n","\n","image_link = \"/content/data/set5\"\n","\n","scale = 4\n","for i in range (1,6):\n","    #open image\n","    image_file = image_link +\"/s\" +str(i)+ \".png\"\n","    image = pil_image.open(image_file)\n","    image_width = image.width\n","    image_height = image.height\n","\n","    #create low resolution images\n","    image = image.resize((image.width//scale, image.height//scale), resample=pil_image.BICUBIC)\n","    image.save( image_link +\"/s\"+ str(i)+\"lr\" + str(scale) + \".png\")"],"metadata":{"id":"pzfBMxJ3-jC9","executionInfo":{"status":"ok","timestamp":1678177359063,"user_tz":-540,"elapsed":369,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","\n","#set14 testset preparation\n","\n","\n","image_link = \"/content/data/set14\"\n","\n","scale = 4\n","for i in range (1,15):\n","    #open image\n","    image_file = image_link +\"/ss\" +str(i)+ \".png\"\n","    image = pil_image.open(image_file)\n","    image_width = image.width\n","    image_height = image.height\n","\n","    #create low resolution images\n","    image = image.resize((image.width//scale, image.height//scale), resample=pil_image.BICUBIC)\n","    image.save( image_link +\"/ss\"+ str(i)+\"lr\" + str(scale) + \".png\")"],"metadata":{"id":"hhz203tZKoae","executionInfo":{"status":"ok","timestamp":1678177396384,"user_tz":-540,"elapsed":791,"user":{"displayName":"Justin Yeo","userId":"13989330688041865378"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import PIL.Image as pil_image\n","import numpy as np\n","from torchvision import transforms\n","\n","#Create Empty array\n","hrarr = []\n","lrarr = []\n","image_link = \"/content/data/data/DIV2K_train_HR/0\"\n","\n","#Random transform ( Rotation & Crop & Flip)\n","crop_transform = transforms.Compose([\n","    transforms.RandomRotation(degrees = (0,360)),\n","    transforms.RandomCrop((96,96)),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.RandomHorizontalFlip(p=0.5)\n","       ])\n","\n","for i in range (0,800):\n","    if(i>98):\n","      hrimage = pil_image.open(image_link + str(i+1) + \".png\")\n","    elif(i>8):\n","      hrimage = pil_image.open(image_link + \"0\" + str(i+1) + \".png\")\n","    else:\n","      hrimage = pil_image.open(image_link + \"00\" + str(i+1) + \".png\")\n","\n","    for _ in range(0,10):\n","        #Transform & Crop Image\n","        hrcropped = crop_transform(hrimage) \n","        hrcroppedimage = np.array(hrcropped)\n","\n","        #Create Low-Resolution Image\n","        lrcropped = hrcropped.resize((24,24), resample=pil_image.BICUBIC)\n","        lrcroppedimage = np.array(lrcropped)\n","\n","        #Append patches to array\n","        hrarr.append(hrcroppedimage) \n","        lrarr.append(lrcroppedimage)\n","\n","print(len(hrarr))"],"metadata":{"id":"fYDLC-Fg6Qil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.optim as optim\n","from torch.utils.data.dataloader import DataLoader\n","from PIL import Image\n","from numpy import asarray\n","import random\n","import PIL\n","import albumentations as at\n","from torchvision.models.feature_extraction import create_feature_extractor\n","import torchvision.models as models\n","\n","\n","#Return psnr value between torch.tensor images\n","def psnr_between_rgb(img1,img2):\n","    if len(img1.shape) == 4:\n","        img1 = img1.squeeze(0)\n","        img2 = img2.squeeze(0)\n","    y1 = 16. + (64.738 * img1[0, :, :] + 129.057 * img1[1, :, :] + 25.064 * img1[2, :, :]) / 256.\n","    y2 = 16. + (64.738 * img2[0, :, :] + 129.057 * img2[1, :, :] + 25.064 * img2[2, :, :]) / 256.\n","    psnr = (10. * torch.log10(1. / torch.mean((y1 - y2) ** 2)))\n","    return psnr\n","\n","\n","#Discriminator model\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.layer1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(3,64,kernel_size=3,padding=1),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer1[0].weight)\n","        \n","        self.layer2 = torch.nn.Sequential(\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride = 2, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(64),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer2[0].weight)\n","\n","        self.layer3 = torch.nn.Sequential(\n","            torch.nn.Conv2d(64, 128, kernel_size=3, stride = 1, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(128),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer3[0].weight)\n","\n","        self.layer4 = torch.nn.Sequential(\n","            torch.nn.Conv2d(128, 128, kernel_size=3, stride = 2, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(128),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer4[0].weight)\n","\n","        self.layer5 = torch.nn.Sequential(\n","            torch.nn.Conv2d(128, 256, kernel_size=3, stride = 1, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(256),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer5[0].weight)\n","\n","        self.layer6 = torch.nn.Sequential(\n","            torch.nn.Conv2d(256, 256, kernel_size=3, stride = 2, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(256),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer6[0].weight)\n","\n","        self.layer7 = torch.nn.Sequential(\n","            torch.nn.Conv2d(256, 512, kernel_size=3, stride = 1, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(512),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer7[0].weight)\n","\n","        self.layer8 = torch.nn.Sequential(\n","            torch.nn.Conv2d(512, 512, kernel_size=3, stride = 2, padding=1, bias=False),\n","            torch.nn.BatchNorm2d(512),\n","            torch.nn.LeakyReLU(0.2, True),\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer8[0].weight)\n","        \n","        self.layer9 = torch.nn.Sequential(\n","            torch.nn.Linear(512 * 6 * 6, 1024),\n","            torch.nn.LeakyReLU(0.2, True),\n","            torch.nn.Linear(1024, 1),\n","            torch.nn.Sigmoid()\n","        )\n","        torch.nn.init.kaiming_normal_(self.layer9[0].weight)\n","        torch.nn.init.kaiming_normal_(self.layer9[2].weight)\n","\n","    def forward(self,x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = torch.flatten(out, 1)\n","        out = self.layer9(out)\n","        return out\n","\n","\n","#VGG-19 model\n","class vgg(nn.Module):\n","    def __init__(self):\n","        super(vgg,self).__init__()\n","\n","        model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n","        self.feature_extractor = create_feature_extractor(model, [\"features.35\"])\n","        self.feature_extractor.eval()\n","\n","        self.normalize = transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n","\n","        for model_parameters in self.feature_extractor.parameters():\n","            model_parameters.requires_grad = False\n","        self.content_criterion = torch.nn.MSELoss()\n","\n","    def forward(self, sr, hr):\n","\n","        sr = self.normalize(sr)\n","        hr = self.normalize(hr)\n","\n","        sr = self.feature_extractor(sr)[\"features.35\"]\n","        hr = self.feature_extractor(hr)[\"features.35\"]\n","\n","        return self.content_criterion(sr, hr)\n","\n","\n","# Residual block for Generator\n","class Residual(nn.Module):\n","    \n","    def __init__(self):\n","        super(Residual, self).__init__()\n","        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, padding=1,bias = False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.prelu = nn.PReLU()\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1,bias = False)\n","        self.bn2 = nn.BatchNorm2d(64)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(residual)\n","        out = self.bn1(out)\n","        out = self.prelu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        return out + residual\n","\n","\n","# Generator model\n","class Generator(nn.Module):\n","    \n","    def __init__(self):\n","        super(Generator,self).__init__()\n","\n","        self.block1 = nn.Sequential( \n","            nn.Conv2d(3,64,kernel_size=9, padding =4),\n","            nn.PReLU()\n","        )\n","\n","        self.residuals = self.stack_residual(Residual,16)\n","        \n","        self.block3 = nn.Sequential(\n","            nn.Conv2d(64,64, kernel_size = 3, padding = 1),\n","            nn.BatchNorm2d(64)\n","        )\n","\n","        self.block4 = nn.Sequential(\n","            nn.Conv2d(64, 256, kernel_size = 3, padding = 1),\n","            nn.PixelShuffle(2),\n","            nn.PReLU()\n","        )\n","        \n","        self.block5 = nn.Sequential(\n","            nn.Conv2d(64, 256, kernel_size =3, padding = 1),\n","            nn.PixelShuffle(2),\n","            nn.PReLU()\n","        )\n","\n","        self.block6 = nn.Conv2d(64,3,kernel_size = 9,padding = 4)\n","\n","        self.initialize_weights()\n","\n","    def stack_residual(self,block,num_layer):\n","        layers = []\n","        for i in range(num_layer):\n","            layers.append(block())\n","        return nn.Sequential(*layers)\n","\n","    def initialize_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d):\n","                nn.init.kaiming_normal_(module.weight)\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","            elif isinstance(module, nn.BatchNorm2d):\n","                nn.init.constant_(module.weight, 1)\n","\n","    def forward(self,x):\n","        residual = self.block1(x)\n","        out = self.residuals(residual)\n","        out = torch.add(self.block3(out),residual)\n","        out = self.block4(out)\n","        out = self.block5(out)\n","        out = self.block6(out)\n","        return out\n","\n","\n","\n","####################################################################################\n","######## Mode\n","###      Train new SRResNet      = 1\n","###   Train existing SRResNet    = 2\n","###         Train SRGAN          = 3\n","###        Download output       = 4\n","\n","\n","\n","mode = 1\n","\n","\n","\n","##########  MODEL\n","model_parameters_path = \"/content/data/model/model.pth\"\n","d_model_parameters_path = \"/content/data/model/d_modelpth\"\n","\n","######### PARAMETER\n","learning_rate = 0.0001\n","training_epochs = 1\n","batchsize = 16\n","userseed = 123\n","\n","######### FILE ROUTES\n","sub_image_size = 96\n","scale = 4\n","image_link = \"/content/data/data/DIV2K_train_HR/0\"\n","test_image_link = \"/content/data/set14/ss11lr4.png\"\n","test_image_output_link = \"/content/data/output/output11.png\"\n","\n","#####################################################################################\n","\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","torch.manual_seed(userseed)\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(userseed)\n","\n","#models\n","model = Generator().to(device)\n","d_model = Discriminator().to(device)\n","vgg_model = vgg().to(device)\n","\n","#loading model state\n","if mode ==2 or mode ==3 or mode==4:\n","    model.load_state_dict(torch.load(model_parameters_path))\n","    d_model.load_state_dict(torch.load(d_model_parameters_path))\n","\n","\n","criterion = nn.MSELoss()\n","d_criterion = nn.BCELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","g_optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","d_optimizer = torch.optim.Adam(d_model.parameters(), lr = learning_rate)\n","\n","class TrainDataset(Dataset):\n","    def __init__(self):\n","        super(TrainDataset,self).__init__()\n","    def __getitem__(self,idx):\n","        return lrarr[idx].transpose((2,0,1)).astype(np.float32)/255. ,hrarr[idx].transpose((2,0,1)).astype(np.float32)/255.\n","    def __len__(self):\n","        return len(hrarr) \n","\n","train_dataset = TrainDataset()\n","\n","train_dataloader = DataLoader(dataset = train_dataset,\n","                 batch_size = batchsize,\n","                 shuffle = True,\n","                 num_workers = 2,\n","                 pin_memory=True,\n","                 drop_last = True)\n","\n","#train SRResnet\n","if mode ==1 or mode==2:\n","\n","  for epoch in range(training_epochs):\n","        model.train()\n","        avg_cost=0\n","        for data in train_dataloader:\n","\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            preds = model(inputs)\n","        \n","            loss = criterion(preds, labels)     \n","            optimizer.zero_grad()\n","\n","            loss.backward()\n","\n","            optimizer.step()\n","            avg_cost += loss / batchsize\n","\n","\n","        print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n","        model.eval()\n","        sum_psnr = 0\n","\n","        for i in range(1,6):\n","\n","          setlrlr = Image.open(\"/content/data/set5/s\" + str(i) + \"lr4.png\")\n","          setlrlr = np.array(asarray(setlrlr)).astype(np.float32)\n","          setlrlr = setlrlr.transpose((2,0,1))\n","          setlrlr /= 255.\n","          setlrlr = torch.from_numpy(setlrlr).to(device)\n","          setlrlr = setlrlr.unsqueeze(0)\n","          \n","\n","          sethrhr = Image.open(\"/content/data/set5/s\"+str(i)+\".png\")\n","          sethrhr= np.array(asarray(sethrhr)).astype(np.float32)\n","          sethrhr = sethrhr.transpose((2,0,1))\n","          sethrhr /= 255.\n","          sethrhr = torch.from_numpy(sethrhr).to(device)\n","          sethrhr = sethrhr.unsqueeze(0)\n","\n","          with torch.no_grad():\n","              setpreds = model(setlrlr).clamp(0.0,1.0)\n","          \n","       \n","          psnr = psnr_between_rgb(sethrhr,setpreds)\n","          sum_psnr+=psnr\n","          \n","        print('PSNR: {:.2f}'.format(sum_psnr/5))\n","\n","#Train SRGAN\n","elif mode ==3:\n","    for epoch in range(training_epochs):\n","        model.train()\n","        d_model.train()\n","        avg_d_cost=0\n","        avg_g_cost = 0\n","\n","        for data in train_dataloader:\n","\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","    \n","            #Labels\n","            real_label = torch.full([batchsize,1], 1.0).to(device)\n","            fake_label = torch.full([batchsize,1], 0.0).to(device)\n","\n","\n","\n","            #### Generator\n","            for d_parameters in d_model.parameters():\n","                d_parameters.requires_grad = False\n","\n","            d_optimizer.zero_grad()\n","            optimizer.zero_grad()\n","\n","            sr = model(inputs)\n","            ad_loss = d_criterion(d_model(sr), real_label)\n","\n","            vgg_loss = vgg_model(sr,labels)\n","            perceptual_loss = 0.06*vgg_loss + 0.001*ad_loss     \n","            perceptual_loss.backward()\n","            optimizer.step()\n","\n","            for d_parameters in d_model.parameters():\n","                d_parameters.requires_grad = True\n","\n","\n","\n","            ### Discriminator\n","            d_optimizer.zero_grad()\n","            optimizer.zero_grad()\n","\n","            d_real_output = d_model(labels)\n","            d_real_loss = d_criterion(d_real_output,real_label)\n","\n","            \n","            sr = model(inputs)\n","            d_fake_output = d_model(sr)\n","            d_fake_loss = d_criterion(d_fake_output, fake_label)\n","\n","\n","            d_loss = d_real_loss + d_fake_loss\n","            d_loss = d_loss/2.0\n","            d_loss.backward()\n","            d_optimizer.step()\n","\n","            avg_d_cost += d_loss / batchsize\n","            avg_g_cost += perceptual_loss / batchsize\n","\n","        \n","        print('[Epoch: {:>4}] d_cost = {:>.9} g_cost = {:>.9}'.format(epoch + 1, avg_d_cost, avg_g_cost))\n","\n","    \n","        model.eval()\n","              \n","\n","\n","##generate test image\n","elif mode==4:\n","    \n","    model.eval()\n","\n","    setlrlr = Image.open(test_image_link)\n","    setlrlr= np.array(asarray(setlrlr)).astype(np.float32)\n","    setlrlr = setlrlr.transpose((2,0,1))\n","    setlrlr /= 255.\n","    setlrlr = torch.from_numpy(setlrlr).to(device)\n","    setlrlr = setlrlr.unsqueeze(0)\n","    with torch.no_grad():\n","        setpreds = model(setlrlr).clamp(0.0,1.0)\n","\n","    #save part \n","    preds2 = setpreds.mul(255.0).cpu().numpy()\n","    preds2 = preds2.squeeze()\n","    preds2 = preds2.transpose((1,2,0))\n","    preds2 = np.clip(preds2, 0.0,255.0).astype(np.uint8)\n","    output = pil_image.fromarray(preds2)\n","    output.save(test_image_output_link)\n","\n","\n","print(\"done\")\n","torch.save(model.state_dict(), model_parameters_path)\n","torch.save(d_model.state_dict(), d_model_parameters_path)"],"metadata":{"id":"yNGJszafM-gg"},"execution_count":null,"outputs":[]}]}